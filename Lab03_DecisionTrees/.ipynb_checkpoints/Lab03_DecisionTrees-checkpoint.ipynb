{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExZu7Yy0r8b4"
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "In this lab you will learn the most important aspects of the decision tree learning method. \n",
    "Completing this lab and analyzing the code will give you a deeper understanding of these type of models.\n",
    "In our experiments we will mostly use the package sklearn from which we import `DecisionTreeClassifier`.\n",
    "\n",
    "## Problem Setting\n",
    "\n",
    "First of all, we calculate by hand the impurity measure of splitting some data by some criteria ${\\color{Lavender}q}$ which in our case is given by a threshold ${\\color{Lavender}t}$. Remember, that information gain ${\\color{CornflowerBlue}IG}({\\color{Lavender}q})$ is calculated as follows: \n",
    "$${\\color{CornflowerBlue}IG}({\\color{Lavender}q}) = {\\color{LimeGreen}S}_{0} - \n",
    "\\mathop{\\color{CornflowerBlue}\\sum}_{{\\color{VioletRed}i}=1}^{k}\\frac{{\\color{CornflowerBlue}N}_{\\color{VioletRed}i}({\\color{Lavender}q})}{N}{\\color{LimeGreen}S}_{\\color{VioletRed}i}({\\color{Lavender}q})$$\n",
    "where $k$ is the number of groups after partition; $N$ is the total number of examples; ${\\color{CornflowerBlue}N}_{\\color{VioletRed}i}({\\color{Lavender}q})$ is the number of examples in the group ${\\color{VioletRed}i}$ and ${\\color{LimeGreen}S}$ is the (Shannon) entropy function defined as:\n",
    "$${\\color{LimeGreen}S} = - \\mathop{\\color{CornflowerBlue}\\sum}_{{\\color{VioletRed}i}}\\,\\hat{\\color{RawSienna}p}_{\\color{VioletRed}i}\\,{\\color{CornflowerBlue}\\log}(\\hat{\\color{RawSienna}p}_{\\color{VioletRed}i})\\;,$$\n",
    "where $\\hat{\\color{RawSienna}p}_{\\color{VioletRed}i}$ is the empirical frequency that the randomly chosen object has label ${\\color{VioletRed}i}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VGqKx3iFr8b6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipycanvas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pi\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mipycanvas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Canvas\n\u001b[1;32m      5\u001b[0m canvas \u001b[38;5;241m=\u001b[39m Canvas(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m      7\u001b[0m canvas\u001b[38;5;241m.\u001b[39mfill_style \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipycanvas'"
     ]
    }
   ],
   "source": [
    "from math import pi\n",
    "\n",
    "from ipycanvas import Canvas\n",
    "\n",
    "canvas = Canvas(width=200, height=200)\n",
    "\n",
    "canvas.fill_style = \"red\"\n",
    "canvas.stroke_style = \"blue\"\n",
    "\n",
    "canvas.fill_arc(60, 60, 50, 0, pi)\n",
    "canvas.stroke_circle(60, 60, 4)\n",
    "\n",
    "canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Output\n",
    "\n",
    "out = Output()\n",
    "\n",
    "\n",
    "@out.capture()\n",
    "def handle_mouse_move(x, y):\n",
    "    print(\"Mouse move event:\", x, y)\n",
    "\n",
    "\n",
    "canvas.on_mouse_move(handle_mouse_move)\n",
    "\n",
    "\n",
    "@out.capture()\n",
    "def handle_mouse_down(x, y):\n",
    "    print(\"Mouse down event:\", x, y)\n",
    "    canvas.stroke_circle(x, y, 4)\n",
    "\n",
    "\n",
    "canvas.on_mouse_down(handle_mouse_down)\n",
    "\n",
    "#display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c6a365a49a448bba30fa57526678a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "out = Output()\n",
    "\n",
    "\n",
    "@out.capture()\n",
    "def handle_touch_move(fingers_locations):\n",
    "    # Draw circles where fingers are located\n",
    "    for finger_location in fingers_locations:\n",
    "        canvas.fill_arc(finger_location[0], finger_location[1], 6, 0, 2 * math.pi)\n",
    "\n",
    "\n",
    "canvas.on_touch_move(handle_touch_move)\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFtm8wZgr8b7"
   },
   "source": [
    "## Entropy\n",
    "<b>Exercise 1:</b>\n",
    "\n",
    "Write a function that calculates the entropy defined as: \n",
    "\n",
    "$${\\color{LimeGreen}S} = - \\mathop{\\color{CornflowerBlue}\\sum}_{{\\color{VioletRed}i}}\\,\\hat{\\color{RawSienna}p}_{\\color{VioletRed}i}\\,{\\color{CornflowerBlue}\\log}(\\hat{\\color{RawSienna}p}_{\\color{VioletRed}i})\\;.$$ \n",
    "\n",
    "<b>Hint:</b> The function input is a vector of labels and the function should return the entropy. (Use the logarithm with base 2. In principle, any base for the logarithm can be used. Changing the base will just change the unit of information. Computer scientists like to use the base 2. In this case information is measured in bits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EqkvzsZRr8b7"
   },
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "    p=[0,0]\n",
    "    for j in [0,1]:\n",
    "        for i in range(len(y)):\n",
    "            if j == y[i]:\n",
    "                p[j]=p[j]+1\n",
    "            #print(p[j])\n",
    "        p[j]=p[j]/len(y)\n",
    "        #print(p[j])\n",
    "    S = 0\n",
    "    for i in range(len(p)):\n",
    "        if p[i] != 0:\n",
    "            S=S+p[i]*np.log2(p[i])\n",
    "            #print(i)\n",
    "            #print(p[i])\n",
    "            print(S)\n",
    "    return -S\n",
    "#entropy(np.array([1,1,0,0])) == 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrgO1GqQr8b9"
   },
   "source": [
    "Check if your solution is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mWeK3pkrr8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5\n",
      "-1.0\n",
      "0.0\n",
      "-0.5283208335737187\n",
      "-0.9182958340544896\n",
      "Testing successful.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert entropy(np.array([1,1,0,0])) == 1.\n",
    "    assert entropy(np.array([0,0])) == 0.\n",
    "    assert str(entropy(np.array([1,1,0,0,1,1])))[:8] == \"0.918295\"\n",
    "    print(\"Testing successful.\")\n",
    "except:\n",
    "    print(\"Tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIY1ypMkr8b-"
   },
   "source": [
    "## Information Gain\n",
    "\n",
    "<b> Exercise 2:</b>\n",
    "Write a function that calculates the information gain obtained by setting the threshold ${\\color{Lavender}t} \\in \\mathbb{R}$ for data points ${\\color{Lavender}x} \\in \\mathbb{R}^{d}$ with label ${\\color{Lavender}y} \\in \\mathbb{N}^{d}$. <b>Hint:</b> The function input is a vector $ {\\color{Lavender}x}$ of attribut values, a vector ${\\color{Lavender}y}$ of labels and a threshold ${\\color{Lavender}t}$. The function should return the information gain ${\\color{CornflowerBlue}IG}({\\color{Lavender}q})$ for that specific split.\n",
    "\n",
    "<b>Example</b>: A split at threshold ${\\color{Lavender}t}=2$ for data points ${\\color{Lavender}x} = [2,4,3,1,5]$ and label ${\\color{Lavender}y}=[1,0,0,0,0]$ would result in the lists \n",
    "* ${\\color{Lavender}x}_{\\text{right}} = [4,3,5]$, ${\\color{Lavender}y}_{\\text{right}} = [0,0,0]$ (for ${\\color{Lavender}x} > {\\color{Lavender}t}$) and\n",
    "* ${\\color{Lavender}x}_{\\text{left}} = [2,1]$, ${\\color{Lavender}y}_{\\text{left}} = [1,0]$ (for ${\\color{Lavender}x} <= {\\color{Lavender}t}$).\n",
    "* Computation of information gain: `entropy([1,0,0,0,0]) - ((0.4 * entropy([1,0]))+(0.6 * entropy([0,0,0])))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Za_MVzVrr8b-"
   },
   "outputs": [],
   "source": [
    "def info_gain(x,y,t):\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ-CFkkyr8b-"
   },
   "source": [
    "Check if your solution is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3rvnvO2r8b_"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    assert str(info_gain(np.array([5,2,3,44,5,6,7,8,9,10]),np.array([1,1,1,1,1,0,0,0,0,0]),5))[:8] == \"0.609986\"\n",
    "    assert str(info_gain(np.array([1,2,3,4,5,6,7,8,9,10]),np.array([1,1,1,1,1,0,0,0,0,0]),1))[:8] == \"0.108031\"\n",
    "    assert str(info_gain(np.array([1,2,3,4,5,6,7,8,9,10]),np.array([1,1,1,1,1,0,0,0,0,0]),3))[:8] == \"0.395815\"\n",
    "    assert str(info_gain(np.array([1,2,3,4,5,6,7,8,9,10]),np.array([1,1,1,1,1,0,0,0,0,0]),5))[:8] == \"1.0\"\n",
    "    assert str(info_gain(np.array([1,2,3,4,5,6,7,8,9,10]),np.array([1,1,1,1,1,0,0,0,0,0]),6))[:8] == \"0.609986\"\n",
    "    print(\"Testing successful.\")\n",
    "except:\n",
    "    print(\"Tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWRhzGvtr8b_"
   },
   "source": [
    "<b> Exercise 3:</b>\n",
    "\n",
    "* Create a 2-dimensional data set with the help of the function make_blobs from package sklearn.datasets (the two returned variables should be named 'X' and 'label'). The data set should contain 500 data points with 2 clusters (2 classes). The cluster centers should be located at (0,0) and (3,3).\n",
    "* Visualize the data set. It should look like this:\n",
    "\n",
    "<img src=\"./figures/dataset.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mqmWRQ9Er8b_"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "### WRITE YOUR CODE HERE ###\n",
    "X, label = None, None # You need to modify this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GREPkjrcr8cA"
   },
   "source": [
    "<b>Exercise 4:</b>\n",
    "\n",
    "Write a function `find_best_split(x,y)` that gets two input vectors ${\\color{Lavender}x} \\in \\mathbb{R}^d$ and ${\\color{Lavender}y} \\in \\{0,1\\}^d$. This function should return the threshold ${\\color{Lavender}t} \\in \\mathbb{R}$ with the best split. <b>Hint:</b> Use the function `info_gain` from exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDUSSbnAr8cA"
   },
   "outputs": [],
   "source": [
    "def find_best_split(x,y):\n",
    "    ### WRITE YOUR CODE HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brkY-2b1r8cA"
   },
   "source": [
    "Check if your solution is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtEefdMdr8cB"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    assert find_best_split(np.array([1,2,3,4,5,6,7,8,9,10]),np.array([1,1,1,1,1,0,0,0,0,0])) == 5\n",
    "    assert find_best_split(np.array([1,2,2,4,5,6,7,8,9,10]),np.array([1,1,0,0,1,0,0,1,0,0])) == 8    \n",
    "    print(\"Testing successful.\")\n",
    "except:\n",
    "    print(\"Tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TAKQeQ7r8cC"
   },
   "source": [
    "<b>Exercise 5:</b>\n",
    "\n",
    "* Find the best splitting for the two dimensions of the data set created in exercise 3. Plot the boundaries in a figure. Your figure should look like this:\n",
    "\n",
    "<img src=\"./figures/split.png\" width=\"600\"/>\n",
    "\n",
    "* Which split would you prefer, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0N0OR2lHr8cD"
   },
   "outputs": [],
   "source": [
    "x_best = find_best_split(X[:,0],label)\n",
    "y_best = find_best_split(X[:,1],label)\n",
    "### WRITE YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4JszCTUr8cD"
   },
   "source": [
    "## Train a Decision Tree\n",
    "Now we train a decision tree using the sklearn package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0EDFCibKr8cD"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This DecisionTreeClassifier estimator requires y to be passed, but the target y is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[1;32m      3\u001b[0m clf_tree \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mclf_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Gedankenspeicher/Programme/miniforge3/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Gedankenspeicher/Programme/miniforge3/lib/python3.10/site-packages/sklearn/tree/_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \n\u001b[1;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Gedankenspeicher/Programme/miniforge3/lib/python3.10/site-packages/sklearn/tree/_classes.py:252\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    248\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    249\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    250\u001b[0m )\n\u001b[1;32m    251\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 252\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/Gedankenspeicher/Programme/miniforge3/lib/python3.10/site-packages/sklearn/base.py:611\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m     )\n\u001b[1;32m    616\u001b[0m no_val_X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m X \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    617\u001b[0m no_val_y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m y \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: This DecisionTreeClassifier estimator requires y to be passed, but the target y is None."
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "clf_tree.fit(X, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pOhOoo1r8cD"
   },
   "source": [
    "<b>Exercise 6:</b>\n",
    "* Calculate the predictions for all instances from the training set (use the function predict). Which instances are misclassified? Create a plot that shows which instances are correctly classified and which instances are misclassified. This figure should look like this:\n",
    "\n",
    "<img src=\"./figures/dec.png\" width=\"600\"/>\n",
    "\n",
    "* Can you find learning parameters for the decision tree classifier (max_depth,min_samples_leaf,max_leaf_nodes,...) to get a classifier that classifies all instances correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xJJyNClr8cD"
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE ###\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab04_DecisionTrees.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
