{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3YDtSxjptXt"
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "In this exercise you will learn how to implement a feedforward <b style=\"color:lightgreen\">neural network</b> and train it with backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TOF5ehVhptXv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multivariate_normal\n",
    "from numpy.random import uniform\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo6F_ZTVptXw"
   },
   "source": [
    "We define two helper functions `init_toy_data()` and `init_model()` to create a simple <b style=\"color:Brown\">data set</b> to work on and a <b style=\"color:lightgreen\">2 layer neural network</b>. \n",
    "\n",
    "### Helper function `init_toy_data(num_samples, num_features, num_classes, seed=3)`\n",
    "\n",
    "First, we create <b style=\"color:RawSienna\">toy data</b> with categorical labels by sampling from different multivariate <b style=\"color:Cornflowerblue\">normal distributions</b> for each class(one entry of the class label vector $\\mathbf{\\color{Dandelion}y}$). \n",
    "\n",
    "- `num_samples`: number of samples *per class*\n",
    "- `num_features`$={\\color{Tomato}J}$: number of features (excluding bias) - entries of the feature vector $\\mathbf{\\color{Dandelion}x}$\n",
    "- `num_classes`: number of class labels - entries of the class label vector $\\mathbf{\\color{Dandelion}y}$\n",
    "- `X(num_samples*num_classes, num_features)`: matrix ${\\color{Brown}X}({\\color{Tomato}N}, {\\color{Tomato}J})$\n",
    "  \n",
    "${\\color{Tomato}N}=$`num_samples*num_classes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "82g8WBTXptXx"
   },
   "outputs": [],
   "source": [
    "def init_toy_data(num_samples, num_features, num_classes, seed=3):\n",
    "    # num_samples: number of samples *per class*\n",
    "    # num_features: number of features (excluding bias)\n",
    "    # num_classes: number of class labels\n",
    "    # seed: random seed\n",
    "    np.random.seed(seed)\n",
    "    X=np.zeros((num_samples*num_classes, num_features))\n",
    "    Y=np.zeros(num_samples*num_classes)\n",
    "    for c in range(num_classes):\n",
    "        # initialize multivariate normal distribution for this class:\n",
    "        # choose a mean for each feature\n",
    "        means = uniform(low=-10, high=10, size=num_features)\n",
    "        # choose a variance for each feature\n",
    "        var = uniform(low=1.0, high=5, size=num_features)\n",
    "        # for simplicity, all features are uncorrelated (covariance between any two features is 0)\n",
    "        cov = var * np.eye(num_features)\n",
    "        # draw samples from normal distribution\n",
    "        X[num_samples*c:num_samples*(c+1),:] = multivariate_normal(means, cov, size=num_samples)\n",
    "        # set label\n",
    "        Y[num_samples*c:num_samples*(c+1)] = c\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2eip8kX7ptXz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[ 0.39636145  1.09468144 -0.89360845  0.91815536]\n",
      " [ 0.94419323 -0.94027869  1.22268078  1.29597409]\n",
      " [-1.41577399  1.15477931 -0.62099631  0.08323307]\n",
      " [-1.35264614 -0.13598976 -1.14221784  0.26928935]\n",
      " [ 0.9352123   0.38225626  1.419864   -1.51152157]\n",
      " [ 0.49265316 -1.55544856  0.01427781 -1.0551303 ]]\n",
      "Y: [0. 0. 1. 1. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# create toy data\n",
    "X, Y = init_toy_data(2, 4, 3) # 2 samples per class; 4 features, 3 classes\n",
    "# Normalize data\n",
    "X = zscore(X, axis=0)\n",
    "print('X: ' + str(X))\n",
    "print('Y: ' + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpCCDC2mptXz"
   },
   "source": [
    "### Helper function `init_model(input_size, hidden_size, num_classes, seed=3)`\n",
    "\n",
    "We now initialise our <b style=\"color:lightgreen\">neural network</b> with one hidden layer consisting of $10$ units and and an output layer consisting of $3$ units. Here we expect (any number of) training samples with $4$ features. We do not apply any activation functions yet. The following figure shows a graphical representation of this neuronal net. \n",
    "\n",
    "<img src=\"nn.graphviz.png\"  width=\"20%\" height=\"20%\">\n",
    "\n",
    "- `input size`: number of input features - entries of the feature vector $\\mathbf{\\color{Dandelion}x}$\n",
    "- `hidden_size`: number of units in the hidden layer\n",
    "- `num_classes`: number of class labels, i.e., number of output units - entries of the class label vector $\\mathbf{\\color{Dandelion}y}$\n",
    "- `model`: a dictionary of the weight matrices and the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = np.random.uniform(-1,1,10)\n",
    "#print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y28dcb7bptXy"
   },
   "outputs": [],
   "source": [
    "def init_model(input_size, hidden_size, num_classes, seed=3):\n",
    "    # input size: number of input features\n",
    "    # hidden_size: number of units in the hidden layer\n",
    "    # num_classes: number of class labels, i.e., number of output units\n",
    "    np.random.seed(seed)\n",
    "    # Model defined as dictionary\n",
    "    model = {}\n",
    "    # initialize weight matrices and biases randomly\n",
    "    model['W1'] = uniform(low=-1, high=1, size=(input_size, hidden_size))\n",
    "    model['b1'] = uniform(low=-1, high=1, size=hidden_size)\n",
    "    model['W2'] = uniform(low=-1, high=1, size=(hidden_size, num_classes))\n",
    "    model['b2'] = uniform(low=-1, high=1, size=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "99BAxa6RptX0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: {'W1': array([[ 0.10159581,  0.41629565, -0.41819052,  0.02165521,  0.78589391,\n",
      "         0.79258618, -0.74882938, -0.58551424, -0.89706559, -0.11838031],\n",
      "       [-0.94024758, -0.08633355,  0.2982881 , -0.44302543,  0.3525098 ,\n",
      "         0.18172563, -0.95203624,  0.11770818, -0.48149511, -0.16979761],\n",
      "       [-0.43294984,  0.38627584, -0.11909256, -0.68626452,  0.08929804,\n",
      "         0.56062953, -0.38727294, -0.55608423, -0.22405748,  0.8727673 ],\n",
      "       [ 0.95199084,  0.34476735,  0.80566822,  0.69150174, -0.24401192,\n",
      "        -0.81556598,  0.30682181,  0.11568152, -0.27687047, -0.54989099]]), 'b1': array([-0.18696017, -0.0621195 , -0.46152884, -0.41641445, -0.0846272 ,\n",
      "        0.72106783,  0.17250581, -0.43302428, -0.44404499, -0.09075585]), 'W2': array([[-0.58917931, -0.59724258,  0.02807012],\n",
      "       [-0.82554126, -0.03282894, -0.27564758],\n",
      "       [ 0.41537324,  0.49349245,  0.38218584],\n",
      "       [ 0.37836083, -0.25279975,  0.33626961],\n",
      "       [-0.32030267,  0.14558774, -0.34838568],\n",
      "       [-0.1097099 , -0.87694214, -0.51464916],\n",
      "       [ 0.94320521, -0.53883159,  0.38295502],\n",
      "       [ 0.30095372,  0.44787828, -0.04982278],\n",
      "       [ 0.19332755, -0.86606115, -0.85487572],\n",
      "       [-0.60204795, -0.69627801, -0.79979131]]), 'b2': array([-0.74141227,  0.10655546, -0.62437035])}\n",
      "model['W1'].shape: (4, 10)\n",
      "model['W2'].shape: (10, 3)\n",
      "model['b1'].shape: (10,)\n",
      "model['b12'].shape: (3,)\n",
      "number of parameters: 83\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = init_model(input_size=4, hidden_size=10, num_classes=3)\n",
    "\n",
    "print('model: ' + str(model))\n",
    "print('model[\\'W1\\'].shape: ' + str(model['W1'].shape))\n",
    "print('model[\\'b1\\'].shape: ' + str(model['b1'].shape))\n",
    "print('model[\\'W2\\'].shape: ' + str(model['W2'].shape))\n",
    "print('model[\\'b12\\'].shape: ' + str(model['b2'].shape))\n",
    "print('number of parameters: ' + str((model['W1'].shape[0] * model['W1'].shape[1]) + \n",
    "     np.sum(model['W2'].shape[0] * model['W2'].shape[1]) + \n",
    "     np.sum(model['b1'].shape[0]) +\n",
    "     np.sum(model['b2'].shape[0] )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ojs6ScguptX1"
   },
   "source": [
    "## <a name=task1> Exercise 1: Implement softmax layer </a>\n",
    "Implement the <b style=\"color:cornflowerblue\">softmax function</b>`softmax(X)` given by \n",
    "\n",
    "$$\n",
    "\\mathrm{\\color{cornflowerblue}softmax}({\\color{Dandelion}x}_{\\color{VioletRed}i}) = \\frac{e^{{\\color{Dandelion}x}_{\\color{VioletRed}i}}}{{\\mathop{\\color{CornflowerBlue}\\sum}_{{\\color{VioletRed}j}\\in 1...{\\color{Tomato}J}}e^{{\\color{Dandelion}x}_{\\color{VioletRed}j}}}}\\,,\n",
    "$$\n",
    "\n",
    "where ${\\color{Chocolate}J}$ is the total number of classes, so the length of  ${\\color{Dandelion}\\mathbf{x}}$ .\n",
    "\n",
    "Note: Implement the function such that it takes a matrix ${\\color{Cornflowerblue}X}$ of shape $({\\color{Tomato}N}, {\\color{Tomato}J})$ as input rather than a single instance ${\\color{Dandelion}\\mathbf{x}}$; ${\\color{Tomato}N}$ is the number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GcgC5wMvptX1"
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    result=np.zeros((X.shape[0], X.shape[1]))\n",
    "    #print(result)\n",
    "    for m in range(X.shape[0]):\n",
    "        for i in range(X.shape[1]):\n",
    "            #print(m,i)\n",
    "            #print(X[m,i])\n",
    "            #print('-')\n",
    "            if i==0:\n",
    "                sum=0\n",
    "                for j in range(X.shape[1]):\n",
    "                    #print(m,j)\n",
    "                    #print(X[m,j])\n",
    "                    #print('-')\n",
    "                    sum=sum+np.exp(X[m,j])\n",
    "            result[m,i]=(np.exp(X[m,i])/sum)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X[0,:])\n",
    "#print(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-BcVCIqptX2"
   },
   "source": [
    "Check if everything is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "recCBdmqptX2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing successful.\n"
     ]
    }
   ],
   "source": [
    "#XX = np.array([[0.1, 0.7, 0.23],[0.7, 0.4, 0.56]])\n",
    "XX = np.array([[0.1, 0.7],[0.7, 0.4]])\n",
    "exact_softmax = np.array([[ 0.35434369,  0.64565631],\n",
    "                         [ 0.57444252,  0.42555748]])\n",
    "#print(XX[1,2])\n",
    "sm = softmax(XX)\n",
    "\n",
    "#print(sm)\n",
    "difference = np.sum(np.abs(exact_softmax - sm))\n",
    "try:\n",
    "    assert difference < 0.000001   \n",
    "    print(\"Testing successful.\")\n",
    "except:\n",
    "    print(\"Tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bO0gCmA3ptX3"
   },
   "source": [
    "## <b>Exercise 2</b>: Implement the forward propagation algorithm for the model defined above.\n",
    "\n",
    "The <b style=\"color:cornflowerblue\">activation function</b> of the hidden neurons is a <b style=\"color:cornflowerblue\">Rectified Linear Unit</b> `relu(x)`$=\\mathrm{\\color{cornflowerblue}max}(0, {\\color{Dandelion}x})$ (to be applied element-wise to the hidden units)\n",
    "The <b style=\"color:cornflowerblue\">activation function</b> of the output layer is a <b style=\"color:cornflowerblue\">softmax function</b> as (as implemented in [Exercise 1](task1)).\n",
    "\n",
    "The function should return both the activation of the hidden units (after having applied the `relu` <b style=\"color:cornflowerblue\">activation function</b>) (shape: $({\\color{Tomato}N},$ `num_hidden`)) and the softmax model output (shape: $({\\color{Tomato}N},$ `num_classes`)). \n",
    "\n",
    "Hiddenlayer function is linear:\n",
    "\n",
    "\n",
    "Outputlayer function is the softmax function for probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "pze-k4-XptX3"
   },
   "outputs": [],
   "source": [
    "def forward_prop(X, model):\n",
    "    W1=model['W1'] # theta for first layer\n",
    "    b1=model['b1'] # theta_0 bias for first layer\n",
    "    W2=model['W2'] # theta for hidden layer\n",
    "    b2=model['b2'] # theta_0 bias for hidden layer\n",
    "    #x_1=theta*X+theta_0\n",
    "    #x_2=softmax(X)\n",
    "    firstlayer=np.zeros((X.shape[0], b1.shape[0]))\n",
    "    hidden_activations=np.zeros((X.shape[0], b2.shape[0]))\n",
    "    probs=np.zeros((b2.shape[0], X.shape[1]))\n",
    "    for m in range(X.shape[0]):\n",
    "        #print(firstlayer)\n",
    "        #print(hidden_activations)\n",
    "        firstlayer[m]=np.dot(W1.T, X[m,:])+b1 #x_1\n",
    "        hidden_activations[m]=np.dot(W2.T, firstlayer[m])+b2 # x_2\n",
    "    probs=softmax(hidden_activations)\n",
    "    print(probs)\n",
    "    return hidden_activations, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "SHrLsiylptX3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02860799 0.72168744 0.24970457]\n",
      " [0.02815474 0.21559875 0.75624651]\n",
      " [0.51480197 0.38142659 0.10377143]\n",
      " [0.83774096 0.02534048 0.13691856]\n",
      " [0.01346719 0.96908424 0.01744856]\n",
      " [0.64124853 0.072682   0.28606947]]\n",
      "Tests failed.\n"
     ]
    }
   ],
   "source": [
    "acts, probs = forward_prop(X, model)\n",
    "correct_probs = np.array([[0.22836388, 0.51816433, 0.25347179],\n",
    "                            [0.15853289, 0.33057078, 0.51089632],\n",
    "                            [0.40710319, 0.41765056, 0.17524624],\n",
    "                            [0.85151353, 0.03656425, 0.11192222],\n",
    "                            [0.66016592, 0.19839791, 0.14143618],\n",
    "                            [0.70362036, 0.08667923, 0.20970041]])\n",
    "\n",
    "# the difference should be very small.\n",
    "difference =  np.sum(np.abs(probs - correct_probs))\n",
    "\n",
    "try:\n",
    "    assert probs.shape==(X.shape[0],len(set(Y)))\n",
    "    assert difference < 0.00001   \n",
    "    print(\"Testing successful.\")\n",
    "except:\n",
    "    print(\"Tests failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGef2WaLptX4"
   },
   "source": [
    "## <b>Exercise 3:</b> \n",
    "\n",
    "How would you train the above defined neural network? Which loss-function would you use? You do not need to implement this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSP8x8NEptX4"
   },
   "source": [
    "# <b>Part 2 (Neural Net using Keras)</b>\n",
    "\n",
    "Instead of implementing the model learning ourselves, we can use the neural network library Keras for Python (https://keras.io/). Keras is an abstraction layer that either builds on top of Theano or Google's Tensorflow. So please install Keras and Tensorflow/Theano for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKozKPKmptX4"
   },
   "source": [
    "## <b>Exercise 4:</b> Implement the same model as above using Keras:\n",
    "    \n",
    "    ** 1 hidden layer à 10 units\n",
    "    ** softmax output layer à three units\n",
    "    ** 4 input features\n",
    "    \n",
    "Compile the model using categorical cross-entropy (also referred to as 'softmax-loss') as loss function and using categorical crossentropy together with categorical accuracy as metrics for runtime evaluation during training.\n",
    "\n",
    "Hint 1: Use the Sequential Class API of Keras (https://keras.io/api/models/sequential/ or https://www.tensorflow.org/guide/keras/sequential_model)\n",
    "\n",
    "Hint 2: You can use the Adam optimizer of Keras for the model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZqK7kxhptX5"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Activation\n",
    "\n",
    "# define the model \n",
    "################################################\n",
    "# INSERT YOUR CODE HERE                        #\n",
    "################################################\n",
    "\n",
    "\n",
    "# compile the model\n",
    "################################################\n",
    "# INSERT YOUR CODE HERE                        #\n",
    "################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QhNMmKGptX5"
   },
   "source": [
    "The description of the current network can always be looked at via the summary method. The layers can be accessed via model.layers and weights can be obtained with the method get_weights. Check if your model is as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abzHV5AxptX5"
   },
   "outputs": [],
   "source": [
    "# Check model architecture and initial weights.\n",
    "\n",
    "W_1, b_1 = model.layers[0].get_weights()\n",
    "print(\"First layer weights: %s; shape: %s\" % (W_1,W_1.shape))\n",
    "print(\"First layer bias: %s; shape: %s\" % (b_1,b_1.shape))\n",
    "W_2, b_2 = model.layers[1].get_weights()\n",
    "print(\"Second layer weights: %s; shape: %s\" % (W_2,W_2.shape))\n",
    "print(\"Second layer bias: %s; shape: %s\" % (b_2,b_2.shape))\n",
    "print(\"number of layes: \" + str(len(model.layers)))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "sk8x5Dy0ptX5",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## <b>Exercise 5:</b> Train the model on the toy data set generated below: \n",
    "\n",
    "Hints: \n",
    "\n",
    "* Keras expects one-hot-coded labels \n",
    "\n",
    "* Don't forget to normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4mu3twRptX6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = init_toy_data(1000,4,3, seed=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=67)\n",
    "\n",
    "##################################\n",
    "# INSERT YOUR CODE HERE          #\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np35zqMPjrjo"
   },
   "source": [
    "Compare the test accuracy with the train accuracy. What can you see? Is the model performing well?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "NeuralNetworks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
